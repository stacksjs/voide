<div class="max-w-3xl mx-auto w-full">
    <!-- Thinking/Done indicator - shown above input when processing or just completed -->
    <div @ref="thinkingIndicator" class="hidden mb-3 px-4">
      <!-- Thinking state -->
      <div @ref="thinkingState" class="flex items-center gap-2 text-monokai-gray/70">
        <div class="flex gap-1">
          <span class="w-1.5 h-1.5 bg-monokai-purple rounded-full animate-bounce" style="animation-delay: 0ms"></span>
          <span class="w-1.5 h-1.5 bg-monokai-purple rounded-full animate-bounce" style="animation-delay: 150ms"></span>
          <span class="w-1.5 h-1.5 bg-monokai-purple rounded-full animate-bounce" style="animation-delay: 300ms"></span>
        </div>
        <span class="text-sm italic">thinking...</span>
        <span @ref="thinkingTimer" class="opacity-40 font-mono" style="font-size: 10px">0.0s</span>
      </div>
      <!-- Done state -->
      <div @ref="doneState" class="hidden flex items-center gap-2 text-monokai-green/80">
        <svg class="w-4 h-4" fill="none" stroke="currentColor" stroke-width="2.5" viewBox="0 0 24 24">
          <path stroke-linecap="round" stroke-linejoin="round" d="M5 13l4 4L19 7"/>
        </svg>
        <span class="text-sm">Crunched for</span>
        <span @ref="doneTimer" class="font-mono" style="font-size: 11px">0s</span>
      </div>
    </div>

    <!-- Audio Preview (shown after recording stops) -->
    <div @ref="audioPreview" class="hidden mb-3">
      <div class="bg-monokai-bg-dark border border-monokai-cyan rounded-xl p-3">
        <!-- Audio player row -->
        <div class="flex items-center gap-3">
          <!-- Native audio element -->
          <audio
            @ref="audioPlayer"
            class="flex-1 h-10"
            controls
            style="filter: sepia(20%) saturate(70%) grayscale(1) contrast(99%) invert(12%); border-radius: 8px;"
          ></audio>

          <!-- Delete audio -->
          <button
            @ref="audioDeleteBtn"
            class="w-8 h-8 rounded-lg text-monokai-gray hover:text-monokai-pink hover:bg-monokai-pink/10 flex items-center justify-center transition-colors"
            title="Delete recording"
          >
            <svg class="w-4 h-4" fill="none" stroke="currentColor" stroke-width="2" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" d="M19 7l-.867 12.142A2 2 0 0116.138 21H7.862a2 2 0 01-1.995-1.858L5 7m5 4v6m4-6v6m1-10V4a1 1 0 00-1-1h-4a1 1 0 00-1 1v3M4 7h16"/>
            </svg>
          </button>
        </div>

        <!-- Transcription toggle -->
        <div class="mt-2 pt-2 border-t border-monokai-border/30">
          <button
            @ref="transcriptToggle"
            class="flex items-center gap-2 text-xs text-monokai-gray hover:text-monokai-fg transition-colors"
          >
            <svg @ref="transcriptChevron" class="w-3 h-3 transition-transform" fill="none" stroke="currentColor" stroke-width="2" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" d="M9 5l7 7-7 7"/>
            </svg>
            <span>View transcription</span>
            <span @ref="transcriptStatus" class="text-monokai-purple"></span>
          </button>

          <!-- Transcription text (collapsible) -->
          <div @ref="transcriptContent" class="hidden mt-2">
            <div
              @ref="transcriptText"
              class="w-full bg-monokai-bg text-sm text-monokai-fg border border-monokai-border/50 rounded-lg p-3 min-h-[60px] max-h-[120px] overflow-y-auto whitespace-pre-wrap"
            ></div>
          </div>
        </div>
      </div>
    </div>

    <div
      @ref="inputContainer"
      class="bg-monokai-bg-dark border border-monokai-border rounded-2xl px-4 py-3"
    >
      <!-- Recording mode indicator (replaces textarea when recording) -->
      <div @ref="recordingMode" class="hidden">
        <div class="flex items-center justify-center gap-4 py-2">
          <!-- Recording pulse -->
          <div class="w-3 h-3 bg-monokai-pink rounded-full animate-pulse"></div>
          <!-- Timer -->
          <span @ref="recordingTimer" class="text-2xl font-mono text-monokai-fg">0:00</span>
        </div>
      </div>

      <!-- Input row (hidden when recording) -->
      <div @ref="inputRow" class="flex items-end gap-3">
        <textarea
          @ref="textInput"
          class="flex-1 bg-transparent text-sm text-monokai-fg border-none outline-none resize-none min-h-[24px] max-h-[160px] leading-6 placeholder-monokai-gray"
          placeholder="Type or click mic to record"
          rows="1"
        ></textarea>
      </div>

      <!-- Bottom bar -->
      <div class="flex items-center justify-between mt-3 pt-3 border-t border-monokai-border/50">
        <div class="flex items-center gap-2">
          <!-- Attachment button -->
          <button
            class="w-8 h-8 rounded-lg flex items-center justify-center text-monokai-gray hover:text-monokai-fg hover:bg-monokai-bg transition-colors"
            title="Attach file"
          >
            <svg class="w-4 h-4" fill="none" stroke="currentColor" stroke-width="2" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" d="M12 4.5v15m7.5-7.5h-15"></path></svg>
          </button>

          <!-- Voice/Mic button -->
          <button
            @ref="micButton"
            class="w-8 h-8 rounded-lg flex items-center justify-center text-monokai-gray hover:text-monokai-fg hover:bg-monokai-bg transition-colors"
            title="Record voice message"
          >
            <svg @ref="micIcon" class="w-4 h-4" fill="none" stroke="currentColor" stroke-width="2" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" d="M12 18.75a6 6 0 006-6v-1.5m-6 7.5a6 6 0 01-6-6v-1.5m6 7.5v3.75m-3.75 0h7.5M12 15.75a3 3 0 01-3-3V4.5a3 3 0 116 0v8.25a3 3 0 01-3 3z"></path></svg>
          </button>

          <!-- Volume Level Indicator (5 bars) - shows mic input level -->
          <div @ref="volumeIndicator" class="hidden flex items-end gap-1 h-6 px-2 py-1 bg-monokai-bg rounded-lg">
            <div @ref="volBar1" class="w-1.5 bg-monokai-gray/40 rounded-full transition-all duration-75" style="height: 20%"></div>
            <div @ref="volBar2" class="w-1.5 bg-monokai-gray/40 rounded-full transition-all duration-75" style="height: 40%"></div>
            <div @ref="volBar3" class="w-1.5 bg-monokai-gray/40 rounded-full transition-all duration-75" style="height: 60%"></div>
            <div @ref="volBar4" class="w-1.5 bg-monokai-gray/40 rounded-full transition-all duration-75" style="height: 80%"></div>
            <div @ref="volBar5" class="w-1.5 bg-monokai-gray/40 rounded-full transition-all duration-75" style="height: 100%"></div>
          </div>

          <!-- Stop button (shown when processing) -->
          <button
            @ref="stopBtn"
            class="hidden w-8 h-8 rounded-lg flex items-center justify-center bg-monokai-pink/20 text-monokai-pink hover:bg-monokai-pink/30 transition-colors"
            title="Stop"
          >
            <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24"><rect x="6" y="6" width="12" height="12" rx="2"></rect></svg>
          </button>
        </div>

        <div class="flex items-center gap-3">
          <!-- Driver selector -->
          <select
            @ref="driverSelect"
            class="bg-transparent text-sm text-monokai-fg/70 border-none outline-none cursor-pointer"
          >
            <option value="claude-sdk">Claude</option>
            <option value="openai">OpenAI</option>
            <option value="ollama">Ollama</option>
          </select>

          <!-- Send button -->
          <button
            @ref="sendBtn"
            class="w-8 h-8 rounded-lg bg-monokai-yellow flex items-center justify-center cursor-pointer hover:bg-monokai-yellow/90 transition-colors"
            title="Send (Enter)"
          >
            <svg class="w-4 h-4" fill="none" stroke="#1e1e1e" stroke-width="2.5" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" d="M4.5 10.5L12 3m0 0l7.5 7.5M12 3v18"></path></svg>
          </button>
        </div>
      </div>
    </div>
</div>

<script client>
(() => {
  const { appStore, chatStore, settingsStore } = window.__STX_STORES__
  const stores = window.VoideStores; // For composables
  const API_BASE_URL = 'http://localhost:3008/voide'

  function init() {

    // =========================================================================
    // Refs - Vue-style auto-binding from ref attributes
    // =========================================================================
    const refs = STX.useRefs()

    // Volume bars as array for easy iteration
    const volBars = [
      refs.volBar1, refs.volBar2, refs.volBar3, refs.volBar4, refs.volBar5
    ].filter(Boolean)

    console.log('[VoideInputBar] Initialized with refs')

    // =========================================================================
    // Composables
    // =========================================================================
    const sounds = stores.useAudioCues()
    const voiceCommands = stores.useVoiceCommands()
    const audioStorage = stores.useAudioStorage()

    // Audio recorder with callbacks
    const audioRecorder = stores.useAudioRecorder({
      onVolumeChange: updateVolumeIndicator,
      onDurationChange: updateRecordingTimer,
      onRecordingStop: handleRecordingStop
    })

    // Background transcription using speech recognition
    const speechRecognition = stores.useSpeechRecognition({
      continuous: true,
      interimResults: false,
      lang: 'en-US'
    })

    // =========================================================================
    // State
    // =========================================================================
    let isRecording = false
    let hasRecording = false
    let currentAudioUrl = null
    let currentTranscript = ''
    let isTranscriptVisible = false
    let handsFreeModeActive = false; // Track if we're in continuous voice mode

    // =========================================================================
    // Volume Indicator
    // =========================================================================
    function updateVolumeIndicator(level) {
      const activeCount = Math.ceil(level / 2)
      volBars.forEach((bar, i) => {
        if (bar) {
          if (i < activeCount) {
            bar.classList.remove('bg-monokai-gray/40')
            bar.classList.add('bg-monokai-cyan')
          } else {
            bar.classList.remove('bg-monokai-cyan')
            bar.classList.add('bg-monokai-gray/40')
          }
        }
      })
      if (level > 0) {
        console.log('[Voide] Mic level:', level, '/ 10')
      }
    }

    // =========================================================================
    // Recording Timer
    // =========================================================================
    function updateRecordingTimer(seconds) {
      const timer = refs.recordingTimer
      if (timer) {
        const mins = Math.floor(seconds / 60)
        const secs = seconds % 60
        timer.textContent = `${mins}:${secs.toString().padStart(2, '0')}`
      }
    }

    // =========================================================================
    // Recording Flow
    // =========================================================================
    async function startRecording() {
      if (isRecording) return

      const success = await audioRecorder.start()
      if (!success) {
        console.error('[Voide] Failed to start recording')
        return
      }

      isRecording = true
      hasRecording = false
      currentTranscript = ''
      handsFreeModeActive = true; // Enable hands-free mode

      // Update UI using refs
      refs.recordingMode?.classList.remove('hidden')
      refs.inputRow?.classList.add('hidden')
      refs.volumeIndicator?.classList.remove('hidden')
      refs.audioPreview?.classList.add('hidden')

      const container = refs.inputContainer
      if (container) {
        container.classList.remove('border-monokai-border')
        container.classList.add('border-monokai-pink')
      }

      const micBtn = refs.micButton
      if (micBtn) {
        micBtn.classList.remove('text-monokai-gray', 'hover:text-monokai-fg', 'hover:bg-monokai-bg')
        micBtn.classList.add('text-monokai-pink', 'bg-monokai-pink/20')
      }

      refs.micIcon?.setAttribute('stroke', '#ff6188')

      startBackgroundTranscription()
      sounds.listening()
    }

    async function stopRecording() {
      if (!isRecording) return

      const blob = await audioRecorder.stop()
      isRecording = false
      stopBackgroundTranscription()

      if (blob) {
        hasRecording = true
        currentAudioUrl = URL.createObjectURL(blob)

        const player = refs.audioPlayer
        if (player) {
          player.src = currentAudioUrl
          player.load()
        }

        refs.audioPreview?.classList.remove('hidden')

        // Show transcript immediately
        const transcriptTextEl = refs.transcriptText
        if (transcriptTextEl) {
          transcriptTextEl.textContent = currentTranscript || ''
        }

        // Auto-expand transcript section if we have content
        if (currentTranscript) {
          refs.transcriptContent?.classList.remove('hidden')
          refs.transcriptChevron?.classList.add('rotate-90')
          isTranscriptVisible = true
        }

        updateTranscriptStatus()
        sounds.done()
      }

      // Reset UI
      refs.recordingMode?.classList.add('hidden')
      refs.inputRow?.classList.remove('hidden')
      refs.volumeIndicator?.classList.add('hidden')

      const container = refs.inputContainer
      if (container) {
        container.classList.remove('border-monokai-pink')
        container.classList.add('border-monokai-border')
      }

      const micBtn = refs.micButton
      if (micBtn) {
        micBtn.classList.remove('text-monokai-pink', 'bg-monokai-pink/20')
        micBtn.classList.add('text-monokai-gray', 'hover:text-monokai-fg', 'hover:bg-monokai-bg')
      }

      refs.micIcon?.setAttribute('stroke', 'currentColor')
      updateVolumeIndicator(0)
    }

    function toggleRecording() {
      if (isRecording) {
        // Manual stop - turn off hands-free mode
        handsFreeModeActive = false
        stopRecording()
      } else if (hasRecording) {
        // If we have a recording preview, clicking mic again cancels it
        handsFreeModeActive = false
        deleteRecording()
      } else {
        startRecording()
      }
    }

    function handleRecordingStop(blob, duration) {
      console.log('[Voide] Recording stopped:', duration, 'seconds')
    }

    // =========================================================================
    // Background Transcription & Silence Detection
    // =========================================================================
    let lastTranscriptTime = 0
    let lastTranscriptLength = 0
    let silenceCheckInterval = null
    let transcriptResultHandler = null
    let transcriptEndHandler = null

    function startBackgroundTranscription() {
      if (!speechRecognition.isSupported) return

      currentTranscript = ''
      lastTranscriptTime = Date.now()
      lastTranscriptLength = 0

      // Remove old handlers if they exist
      if (transcriptResultHandler) {
        // Can't remove, but we'll check isRecording
      }

      transcriptResultHandler = (data) => {
        if (!isRecording) return
        if (data.transcript && data.transcript.length > lastTranscriptLength) {
          currentTranscript = voiceCommands.convertPunctuation(data.transcript)
          lastTranscriptLength = data.transcript.length
          lastTranscriptTime = Date.now()

          const transcriptTextEl = refs.transcriptText
          if (transcriptTextEl) {
            transcriptTextEl.textContent = currentTranscript
          }
          updateTranscriptStatus()
          console.log('[Voide] Got words:', currentTranscript.split(' ').length)
        }
      }

      transcriptEndHandler = () => {
        if (isRecording) {
          // Restart recognition to keep listening
          setTimeout(() => {
            if (isRecording) {
              try { speechRecognition.start(); } catch (e) {}
            }
          }, 100)
        }
      }

      speechRecognition.on('result', transcriptResultHandler)
      speechRecognition.on('end', transcriptEndHandler)

      // Start silence detection polling
      startSilenceDetection()

      speechRecognition.start()
    }

    function stopBackgroundTranscription() {
      speechRecognition.stop()
      stopSilenceDetection()
    }

    function startSilenceDetection() {
      stopSilenceDetection(); // Clear any existing

      silenceCheckInterval = setInterval(() => {
        if (!isRecording || !handsFreeModeActive) return

        const silenceDuration = Date.now() - lastTranscriptTime
        const hasContent = currentTranscript.trim().length > 0

        // If we have content and 5 seconds of silence, auto-stop
        if (hasContent && silenceDuration >= 5000) {
          console.log('[Voide] 5s silence detected, auto-stopping recording')
          stopRecording()

          // Show preview for 1.5 seconds, then auto-send
          setTimeout(() => {
            if (hasRecording && currentTranscript.trim()) {
              console.log('[Voide] Auto-sending voice message')
              handleSend()
            }
          }, 1500)
        }
      }, 500); // Check every 500ms
    }

    function stopSilenceDetection() {
      if (silenceCheckInterval) {
        clearInterval(silenceCheckInterval)
        silenceCheckInterval = null
      }
    }

    function updateTranscriptStatus() {
      const status = refs.transcriptStatus
      if (status) {
        if (currentTranscript) {
          status.textContent = `(${currentTranscript.split(' ').length} words)`
        } else {
          status.textContent = isRecording ? '(transcribing...)' : ''
        }
      }
    }

    // =========================================================================
    // Audio Management
    // =========================================================================
    function deleteRecording(keepHandsFreeMode = false) {
      const player = refs.audioPlayer
      if (player) {
        player.pause()
        player.src = ''
      }
      if (currentAudioUrl) {
        URL.revokeObjectURL(currentAudioUrl)
        currentAudioUrl = null
      }
      hasRecording = false
      currentTranscript = ''
      if (!keepHandsFreeMode) {
        handsFreeModeActive = false
      }

      refs.audioPreview?.classList.add('hidden')
      const transcriptTextEl = refs.transcriptText
      if (transcriptTextEl) transcriptTextEl.textContent = ''

      // Reset transcript visibility
      refs.transcriptContent?.classList.add('hidden')
      refs.transcriptChevron?.classList.remove('rotate-90')
      isTranscriptVisible = false

      audioRecorder.reset()
    }

    // =========================================================================
    // Transcript Toggle
    // =========================================================================
    function toggleTranscript() {
      isTranscriptVisible = !isTranscriptVisible

      const content = refs.transcriptContent
      if (content) {
        content.classList.toggle('hidden', !isTranscriptVisible)
      }

      const chevron = refs.transcriptChevron
      if (chevron) {
        chevron.style.transform = isTranscriptVisible ? 'rotate(90deg)' : ''
      }
    }

    // =========================================================================
    // API & Command Processing
    // =========================================================================
    function checkBackendAPI() {
      return fetch(API_BASE_URL + '/state').then(res => res.ok).catch(() => false)
    }

    function saveChat() {
      const app = appStore.$state
      chatStore.saveCurrentChat(app.repoPath, app.currentDriver)
    }

    let saveChatDebounceTimer = null
    function updateLastMessage(type, content) {
      chatStore.updateLastMessage(type, content)
      clearTimeout(saveChatDebounceTimer)
      saveChatDebounceTimer = setTimeout(saveChat, 500)
    }

    function addMessage(type, content, header) {
      const app = appStore.$state
      const driverName = app.currentDriver === 'claude-sdk' ? 'Claude SDK' :
                         app.currentDriver === 'openai' ? 'OpenAI' :
                         app.currentDriver === 'ollama' ? 'Ollama' : 'AI'
      const headerText = header || (type === 'user' ? 'You' : type === 'assistant' ? driverName : type === 'system' ? 'System' : 'Error')
      chatStore.addMessage(type, content, headerText)
      saveChat()
    }

    function detectMode(command) {
      const lower = command.toLowerCase().trim()
      const questionPatterns = [
        /^(what|why|how|when|where|who|which|can you explain|could you explain|please explain|tell me|describe|is it|are there|do you|does|did|will|would|should|could|can|may|might)/i,
        /\?$/,
        /^explain\s/i,
        /^describe\s/i,
        /^tell me\s/i,
        /^help me understand/i,
      ]
      const actionPatterns = [
        /^(fix|create|add|remove|delete|update|change|modify|refactor|implement|write|build|make|generate|run|execute|install|setup|configure|debug|test)/i,
        /^please (fix|create|add|remove|delete|update|change|modify|refactor|implement|write|build|make)/i,
        /\b(in my|in the|this|the) (code|file|project|app|application|repo|repository|codebase)\b/i,
      ]

      for (const pattern of actionPatterns) {
        if (pattern.test(lower)) return 'agentic'
      }
      for (const pattern of questionPatterns) {
        if (pattern.test(lower)) return 'simple'
      }
      return 'agentic'
    }

    function processWithStreaming(command, targetChatId) {
      return new Promise((resolve, reject) => {
        let fullContent = ''
        let currentEvent = ''
        const app = appStore.$state
        const chat = chatStore.$state
        const mode = detectMode(command)

        // Helper to check if we're still on the same chat
        function isStillCurrentChat() {
          return chatStore.$state.currentChatId === targetChatId
        }

        const history = chat.messages.slice(0, -1).map(m => ({
          role: m.type === 'user' ? 'user' : 'assistant',
          content: m.content
        })).filter(m => m.role === 'user' || m.role === 'assistant')

        fetch(API_BASE_URL + '/process/stream', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            command,
            driver: app.currentDriver,
            repository: app.repoPath,
            sessionId: chat.sessionId,
            history,
            mode
          })
        }).then(response => {
          if (!response.ok) throw new Error('Stream request failed')

          const reader = response.body.getReader()
          const decoder = new TextDecoder()
          let buffer = ''

          function readChunk() {
            reader.read().then(result => {
              if (result.done) {
                resolve({ message: fullContent, hasChanges: false, targetChatId })
                return
              }

              // Skip updates if user switched to a different chat
              if (!isStillCurrentChat()) {
                resolve({ message: fullContent, hasChanges: false, targetChatId, switched: true })
                return
              }

              buffer += decoder.decode(result.value, { stream: true })
              const lines = buffer.split('\n')
              buffer = lines.pop() || ''

              for (const line of lines) {
                const trimmed = line.trim()
                if (!trimmed) continue

                if (trimmed.startsWith('event: ')) {
                  currentEvent = trimmed.substring(7)
                  continue
                }

                if (trimmed.startsWith('data: ')) {
                  try {
                    const data = JSON.parse(trimmed.substring(6))

                    if (currentEvent === 'session' && data.sessionId) {
                      if (isStillCurrentChat()) {
                        chatStore.setSessionId(data.sessionId)
                        saveChat()
                      }
                    } else if (currentEvent === 'chunk' && data.text) {
                      fullContent += data.text
                      if (isStillCurrentChat()) {
                        updateLastMessage('assistant', fullContent)
                      }
                    } else if (currentEvent === 'tool' && data.tool) {
                      let toolInfo = data.tool
                      if (data.input) {
                        if (data.tool === 'Read' && data.input.file_path) {
                          toolInfo = 'Reading ' + data.input.file_path.split('/').pop()
                        } else if (data.tool === 'Edit' && data.input.file_path) {
                          toolInfo = 'Editing ' + data.input.file_path.split('/').pop()
                        } else if (data.tool === 'Write' && data.input.file_path) {
                          toolInfo = 'Writing ' + data.input.file_path.split('/').pop()
                        } else if (data.tool === 'Bash' && data.input.command) {
                          toolInfo = '$ ' + data.input.command.substring(0, 40)
                        }
                      }
                      fullContent += '\n`' + toolInfo + '`'
                      if (isStillCurrentChat()) {
                        updateLastMessage('assistant', fullContent)
                      }
                    } else if (currentEvent === 'done') {
                      resolve({ message: fullContent, hasChanges: data.hasChanges || false, targetChatId })
                      return
                    } else if (currentEvent === 'error') {
                      reject(new Error(data.error || 'Stream error'))
                      return
                    }
                  } catch (e) {}
                }
              }
              readChunk()
            }).catch(reject)
          }
          readChunk()
        }).catch(reject)
      })
    }

    function generateChatTitle(chatId, prompt) {
      fetch(API_BASE_URL + '/title', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ prompt })
      })
      .then(res => res.json())
      .then(data => {
        if (data.title) chatStore.setChatTitle(chatId, data.title)
      })
      .catch(() => {})
    }

    function processCommand(command, audioId = null) {
      if (!command.trim()) return

      const chat = chatStore.$state
      const app = appStore.$state
      let isNewChat = false
      let newChatId = null
      const wasVoiceMessage = !!audioId

      const messageContent = audioId ? `[Voice message: ${command}]` : command

      if (!chat.currentChatId) {
        newChatId = chatStore.startNewChat(app.repoPath, app.currentDriver, {
          type: 'user',
          content: messageContent,
          header: audioId ? 'You (voice)' : 'You',
          timestamp: Date.now(),
          audioId
        })
        isNewChat = true
      } else {
        addMessage('user', messageContent, audioId ? 'You (voice)' : 'You')
      }

      // Capture the chatId for this processing session
      const targetChatId = newChatId || chat.currentChatId

      appStore.setTerminalTitle('Processing...')

      if (!app.repoPath) {
        addMessage('error', 'Please enter a repository URL or path first.')
        appStore.setTerminalTitle('Voide - Ready')
        return
      }

      appStore.setProcessing(true, targetChatId)
      addMessage('assistant', '...')

      checkBackendAPI().then(hasBackend => {
        if (hasBackend) {
          return processWithStreaming(command, targetChatId)
        } else if (app.currentDriver === 'mock') {
          return new Promise(resolve => {
            setTimeout(() => {
              resolve({ message: '[Mock] Processing: "' + command + '"', hasChanges: false })
            }, 1500)
          })
        } else {
          return Promise.reject(new Error('Backend API not available.'))
        }
      }).then(response => {
        // Skip post-processing if user switched to a different chat
        if (response.switched || chatStore.$state.currentChatId !== targetChatId) {
          appStore.setProcessing(false)
          return
        }

        if (response.hasChanges) {
          appStore.setHasChanges(true)
          addMessage('system', 'Changes staged.')
        }
        appStore.setTerminalTitle('Voide - Ready')

        if (isNewChat && newChatId) {
          generateChatTitle(newChatId, command)
        }

        // Auto-play response if voice message was sent
        if (wasVoiceMessage) {
          setTimeout(() => {
            if (response.message && window.voide?.speak) {
              window.voide.speak(response.message, 'auto-tts', () => {
                // Restart recording after TTS finishes (hands-free mode)
                if (handsFreeModeActive) {
                  console.log('[Voide] TTS finished, restarting recording')
                  setTimeout(() => startRecording(), 500)
                }
              })
            } else if (handsFreeModeActive) {
              // No message to speak, restart recording anyway
              setTimeout(() => startRecording(), 500)
            }
          }, 400)
        }
      }).catch(error => {
        // Only show error in the original chat
        if (chatStore.$state.currentChatId === targetChatId) {
          chatStore.removeLastMessage()
          addMessage('error', 'Failed: ' + error.message)
          appStore.setTerminalTitle('Voide - Error')
        }

        // Restart recording even on error (hands-free mode)
        if (wasVoiceMessage && handsFreeModeActive) {
          setTimeout(() => startRecording(), 1000)
        }
      }).finally(() => {
        appStore.setProcessing(false)
      })
    }

    // =========================================================================
    // Input Handlers
    // =========================================================================
    function handleInputChange(e) {
      const el = e.target
      el.style.height = 'auto'
      el.style.height = Math.min(el.scrollHeight, 160) + 'px'
      chatStore.setInputText(el.value)
    }

    function handleInputKeydown(e) {
      if (e.key === 'Enter' && !e.shiftKey) {
        e.preventDefault()
        handleSend()
      }
    }

    async function handleSend() {
      if (hasRecording) {
        const transcriptTextEl = refs.transcriptText
        const transcript = transcriptTextEl?.textContent?.trim() || currentTranscript

        if (!transcript) {
          console.log('[Voide] No transcript available yet')
          return
        }

        const state = audioRecorder.get()
        let audioId = null
        if (state.audioBlob && audioStorage.isSupported) {
          try {
            audioId = await audioStorage.save(
              state.audioBlob,
              state.duration,
              transcript,
              chatStore.$state.currentChatId
            )
          } catch (e) {
            console.log('[Voide] Failed to save audio:', e)
          }
        }

        deleteRecording(true); // Keep hands-free mode active
        processCommand(transcript, audioId)
        // Recording will restart after TTS finishes (handled in processCommand)
        return
      }

      const chat = chatStore.$state
      const text = chat.inputText.trim()

      if (text && !appStore.$state.isProcessing) {
        chatStore.clearInput()
        const textInputEl = refs.textInput
        if (textInputEl) {
          textInputEl.value = ''
          textInputEl.style.height = 'auto'
        }
        processCommand(text)
      }
    }

    function cancelProcessing() {
      fetch(API_BASE_URL + '/cancel', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({})
      }).then(() => {
        appStore.setProcessing(false)
        const messages = chatStore.$state.messages
        if (messages.length > 0) {
          const lastMsg = messages[messages.length - 1]
          if (lastMsg?.type === 'assistant') {
            updateLastMessage('assistant', lastMsg.content + '\n\nâ€” *Stopped*')
          }
        }
      }).catch(() => {
        appStore.setProcessing(false)
      })
    }

    // =========================================================================
    // Event Bindings
    // =========================================================================
    refs.micButton?.addEventListener('click', toggleRecording)
    refs.sendBtn?.addEventListener('click', handleSend)
    refs.stopBtn?.addEventListener('click', cancelProcessing)
    refs.textInput?.addEventListener('input', handleInputChange)
    refs.textInput?.addEventListener('keydown', handleInputKeydown)
    refs.audioDeleteBtn?.addEventListener('click', () => deleteRecording())
    refs.transcriptToggle?.addEventListener('click', toggleTranscript)

    // Driver selector
    const driverSelectEl = refs.driverSelect
    const initialDriver = appStore.$state.currentDriver || 'claude-sdk'
    if (driverSelectEl) driverSelectEl.value = initialDriver
    driverSelectEl?.addEventListener('change', (e) => {
      appStore.setDriver(e.target.value)
    })

    // Global keyboard shortcuts - Space to toggle recording (when not in input)
    STX.onKey(' ', (e) => {
      if (!e.repeat) {
        const active = STX.activeElement()
        const isInput = active && (active.tagName === 'INPUT' || active.tagName === 'TEXTAREA')
        if (!isInput) {
          e.preventDefault()
          toggleRecording()
        }
      }
    })

    // =========================================================================
    // State Subscriptions
    // =========================================================================
    let processingTimerInterval = null
    let isThinking = false
    let doneHideTimeout = null
    // Map to store timer start times per chat ID (backup for store)
    const timerStartTimes = {}

    function formatElapsedTime(ms) {
      const totalSeconds = Math.floor(ms / 1000)
      const minutes = Math.floor(totalSeconds / 60)
      const seconds = totalSeconds % 60
      if (minutes > 0) {
        return `${minutes}m ${seconds}s`
      }
      return `${totalSeconds}s`
    }

    function startProcessingTimer() {
      isThinking = true
      const timer = refs.thinkingTimer
      if (timer) timer.textContent = '0.0s'

      // Store start time for the current processing chat
      const chatId = appStore.processingChatId
      if (chatId && !timerStartTimes[chatId]) {
        timerStartTimes[chatId] = Date.now()
      }

      // Clear any existing interval
      if (processingTimerInterval) {
        clearInterval(processingTimerInterval)
      }

      processingTimerInterval = setInterval(() => {
        const timerEl = refs.thinkingTimer
        const currentChatId = appStore.processingChatId
        const startTime = timerStartTimes[currentChatId] || appStore.processingStartTime
        if (timerEl && startTime) {
          const elapsed = ((Date.now() - startTime) / 1000).toFixed(1)
          timerEl.textContent = `${elapsed}s`
        }
      }, 100)
    }

    function stopProcessingTimer() {
      const chatId = appStore.processingChatId
      const startTime = timerStartTimes[chatId] || appStore.processingStartTime
      const elapsedMs = startTime ? Date.now() - startTime : 0

      // Clean up the timer start time for this chat
      if (chatId) {
        delete timerStartTimes[chatId]
      }

      if (processingTimerInterval) {
        clearInterval(processingTimerInterval)
        processingTimerInterval = null
      }
      isThinking = false
      return elapsedMs
    }

    function showDoneState(elapsedMs) {
      const indicator = refs.thinkingIndicator
      const thinkingState = refs.thinkingState
      const doneState = refs.doneState
      const doneTimer = refs.doneTimer

      if (!indicator || !thinkingState || !doneState) return

      // Show done state, hide thinking state
      thinkingState.classList.add('hidden')
      doneState.classList.remove('hidden')
      indicator.classList.remove('hidden')

      // Set the elapsed time
      if (doneTimer) {
        doneTimer.textContent = formatElapsedTime(elapsedMs)
      }

      // Clear any existing timeout
      if (doneHideTimeout) {
        clearTimeout(doneHideTimeout)
      }

      // Hide after 3 seconds
      doneHideTimeout = setTimeout(() => {
        indicator.classList.add('hidden')
        doneState.classList.add('hidden')
        thinkingState.classList.remove('hidden')
      }, 3000)
    }

    function showThinkingState() {
      const indicator = refs.thinkingIndicator
      const thinkingState = refs.thinkingState
      const doneState = refs.doneState

      if (!indicator || !thinkingState || !doneState) return

      // Clear any done timeout
      if (doneHideTimeout) {
        clearTimeout(doneHideTimeout)
        doneHideTimeout = null
      }

      // Show thinking state, hide done state
      thinkingState.classList.remove('hidden')
      doneState.classList.add('hidden')
      indicator.classList.remove('hidden')
    }

    appStore.$subscribe((state) => {
      const indicator = refs.thinkingIndicator
      const currentChatId = chatStore.$state.currentChatId
      const isProcessingThisChat = state.isProcessing && state.processingChatId === currentChatId

      if (indicator) {
        if (isProcessingThisChat) {
          showThinkingState()
          if (!processingTimerInterval) startProcessingTimer()
        } else if (isThinking || state.processingStartTime) {
          // Was processing, now done - show done state
          const elapsedMs = stopProcessingTimer()
          if (elapsedMs > 500) {
            // Only show done state if processing took more than 0.5s
            showDoneState(elapsedMs)
          } else {
            indicator.classList.add('hidden')
          }
        }
      }

      const stopBtnEl = refs.stopBtn
      if (stopBtnEl) {
        stopBtnEl.classList.toggle('hidden', !isProcessingThisChat)
      }

      const driverSelectEl = refs.driverSelect
      if (driverSelectEl && state.currentDriver) {
        driverSelectEl.value = state.currentDriver
      }
    })

    // Update indicator when switching chats
    chatStore.$subscribe((chatState) => {
      const appState = appStore.$state
      const indicator = refs.thinkingIndicator
      const stopBtnEl = refs.stopBtn
      const isProcessingThisChat = appState.isProcessing && appState.processingChatId === chatState.currentChatId

      if (indicator) {
        if (isProcessingThisChat) {
          showThinkingState()
          if (!processingTimerInterval) startProcessingTimer()
        } else {
          indicator.classList.add('hidden')
          if (processingTimerInterval) {
            clearInterval(processingTimerInterval)
            processingTimerInterval = null
          }
        }
      }

      if (stopBtnEl) {
        stopBtnEl.classList.toggle('hidden', !isProcessingThisChat)
      }
    })

    // =========================================================================
    // Expose to window.voide
    // =========================================================================
    window.voide = window.voide || {}
    Object.assign(window.voide, {
      toggleRecording,
      startRecording,
      stopRecording,
      handleSend,
      cancelProcessing,
      processCommand,
      addMessage,
      sounds,
      audioRecorder,
      audioStorage
    })
  }

  init()
})()
</script>
